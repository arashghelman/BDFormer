{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/content/drive/MyDrive/BDFormer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "QLGOu3JJY18L"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arashghelman/Desktop/Masters/Thesis/BDFormer/.conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/arashghelman/Desktop/Masters/Thesis/BDFormer/.conda/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-task dataset -> isic17\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from model.BDFormer import BDFormer\n",
    "from datasets import Skin_Dataset\n",
    "from engine import *\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from utils import *\n",
    "from config_setting import setting_config_multitask as config\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logger and output files are created.\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(config.work_dir + '/')\n",
    "\n",
    "log_dir = os.path.join(config.work_dir, 'log')\n",
    "checkpoint_dir = os.path.join(config.work_dir, 'checkpoints')\n",
    "resume_model = os.path.join(checkpoint_dir, 'latest.pth')\n",
    "outputs = os.path.join(config.work_dir, 'outputs')\n",
    "\n",
    "if not os.path.exists(checkpoint_dir): \n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "if not os.path.exists(outputs): \n",
    "    os.makedirs(outputs)\n",
    "\n",
    "if not os.path.exists(os.path.join(outputs, 'pred_masks')):\n",
    "    os.makedirs(os.path.join(outputs, 'pred_masks'))\n",
    "\n",
    "if not os.path.exists(os.path.join(outputs, 'pred_contours')):\n",
    "    os.makedirs(os.path.join(outputs, 'pred_contours'))\n",
    "\n",
    "global logger\n",
    "logger = get_logger('train', log_dir)\n",
    "log_config_info(config, logger)\n",
    "\n",
    "print('Logger and output files are created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is initalized.\n"
     ]
    }
   ],
   "source": [
    "set_seed(config.seed)\n",
    "gpu_ids = [0]\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print('GPU is initalized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets are loaded.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Skin_Dataset(config, split=\"train\", subset_frac=0.2)\n",
    "train_class_weights = calculate_class_weights(train_dataset.labels)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=config.num_workers)\n",
    "\n",
    "val_dataset = Skin_Dataset(config, split=\"val\")\n",
    "val_class_weights = calculate_class_weights(val_dataset.labels)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=config.num_workers,\n",
    "    drop_last=True)\n",
    "\n",
    "test_dataset = Skin_Dataset(config, split=\"test\")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=config.num_workers,\n",
    "    drop_last=True)\n",
    "\n",
    "print('Datasets are loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinTransformerSys expand initial----depths:[2, 2, 2, 2];depths_decoder:[1, 2, 2, 2];drop_path_rate:0.2;num_classes:1\n",
      "Model and training parameters are configured.\n"
     ]
    }
   ],
   "source": [
    "model = BDFormer(img_size=256, in_channels=3, num_classes=config.num_classes, window_size=8)\n",
    "model = torch.nn.DataParallel(model.cuda(), device_ids=gpu_ids, output_device=gpu_ids[0])\n",
    "\n",
    "criterion = config.criterion\n",
    "optimizer = get_optimizer(config, model)\n",
    "scheduler = get_scheduler(config, optimizer)\n",
    "\n",
    "min_loss = 999\n",
    "start_epoch = 1\n",
    "min_epoch = 1\n",
    "\n",
    "print('Model and training parameters are configured.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4eqh3oZe_o7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------Training----------#\n",
      "iter_num= 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 0/50 [00:30<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(start_epoch, config.epochs + \u001b[32m1\u001b[39m), ncols=\u001b[32m70\u001b[39m):\n\u001b[32m     24\u001b[39m     torch.cuda.empty_cache()\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[43mtrain_one_epoch_multi\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# train_class_weights,\u001b[39;49;00m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m#----------Validation----------#\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     38\u001b[39m     loss = val_one_epoch_multi(\n\u001b[32m     39\u001b[39m             val_loader,\n\u001b[32m     40\u001b[39m             model,\n\u001b[32m   (...)\u001b[39m\u001b[32m     44\u001b[39m             \u001b[38;5;66;03m# val_class_weights\u001b[39;00m\n\u001b[32m     45\u001b[39m             )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Masters/Thesis/BDFormer/engine.py:56\u001b[39m, in \u001b[36mtrain_one_epoch_multi\u001b[39m\u001b[34m(train_loader, model, optimizer, scheduler, epoch, logger, config, scaler)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     55\u001b[39m     pred_seg, pred_contour = model(target_img)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     loss = \u001b[43mCaculate_multi_task_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpred_seg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpred_contour\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_seg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_contour\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m     loss.backward()\n\u001b[32m     63\u001b[39m     optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Masters/Thesis/BDFormer/utils.py:554\u001b[39m, in \u001b[36mCaculate_multi_task_loss\u001b[39m\u001b[34m(pred_seg, pred_contour, target_seg, target_contour, alpha)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mCaculate_multi_task_loss\u001b[39m(\n\u001b[32m    548\u001b[39m         pred_seg, \n\u001b[32m    549\u001b[39m         pred_contour,\n\u001b[32m   (...)\u001b[39m\u001b[32m    552\u001b[39m         alpha\n\u001b[32m    553\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     ce_loss_contour = CrossEntropyLoss(weight=\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m55.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    555\u001b[39m     dice_loss_contour = DiceLoss_Contour(n_classes=\u001b[32m2\u001b[39m)\n\u001b[32m    556\u001b[39m     bce_dice_loss = BceDiceLoss()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Masters/Thesis/BDFormer/.conda/lib/python3.12/site-packages/torch/cuda/__init__.py:293\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    289\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    290\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmultiprocessing, you must use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mspawn\u001b[39m\u001b[33m'\u001b[39m\u001b[33m start method\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    291\u001b[39m     )\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch._C, \u001b[33m\"\u001b[39m\u001b[33m_cuda_getDeviceCount\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTorch not compiled with CUDA enabled\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    295\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    296\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    297\u001b[39m     )\n",
      "\u001b[31mAssertionError\u001b[39m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "if os.path.exists(resume_model):\n",
    "    print('#----------Resuming model and learning parameters----------#')\n",
    "    checkpoint = torch.load(resume_model, map_location=torch.device('cpu'), weights_only=False)\n",
    "    model.module.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "    saved_epoch = checkpoint['epoch']\n",
    "    start_epoch += saved_epoch\n",
    "    min_loss, min_epoch, loss = checkpoint['min_loss'], checkpoint['min_epoch'], checkpoint['loss']\n",
    "\n",
    "    log_info = f'resuming model from {resume_model}. resume_epoch: {saved_epoch}, min_loss: {min_loss:.4f}, min_epoch: {min_epoch}, loss: {loss:.4f}'\n",
    "    logger.info(log_info)\n",
    "\n",
    "\n",
    "print('#----------Training----------#')\n",
    "print('iter_num=', len(train_loader))\n",
    "for epoch in tqdm(range(start_epoch, config.epochs + 1), ncols=70):\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    train_one_epoch_multi(\n",
    "        train_loader,\n",
    "        train_class_weights,\n",
    "        model,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        epoch,\n",
    "        logger,\n",
    "        config)\n",
    "\n",
    "    print('#----------Validation----------#')\n",
    "    loss = val_one_epoch_multi(\n",
    "            val_loader,\n",
    "            val_class_weights,\n",
    "            model,\n",
    "            epoch,\n",
    "            logger,\n",
    "            config)\n",
    "\n",
    "    if loss < min_loss:\n",
    "        torch.save(model.module.state_dict(), os.path.join(checkpoint_dir, 'best.pth'))\n",
    "        min_loss = loss\n",
    "        min_epoch = epoch\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            'epoch': epoch,\n",
    "            'min_loss': min_loss,\n",
    "            'min_epoch': min_epoch,\n",
    "            'loss': loss,\n",
    "            'model_state_dict': model.module.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict()\n",
    "        }, os.path.join(checkpoint_dir, 'latest.pth'))\n",
    "\n",
    "if os.path.exists(os.path.join(checkpoint_dir, 'best.pth')):\n",
    "    print('#----------Testing----------#')\n",
    "    best_weight = torch.load(config.work_dir + '/checkpoints/best.pth', map_location=torch.device('cpu'))\n",
    "    model.module.load_state_dict(best_weight)\n",
    "    loss = test_one_epoch_multi(\n",
    "            test_loader,\n",
    "            model,\n",
    "            criterion,\n",
    "            logger,\n",
    "            config)\n",
    "    os.rename(\n",
    "        os.path.join(checkpoint_dir, 'best.pth'),\n",
    "        os.path.join(checkpoint_dir, f'best-epoch{min_epoch}-loss{min_loss:.4f}.pth')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing layers\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# for name, layer in model.multi_task_MaxViT.named_children():\n",
    "#     if name in ['classifier', 'seg_out_conv']:\n",
    "#         for param in layer.parameters():\n",
    "#             param.requires_grad = True\n",
    "\n",
    "# resume_model = os.path.join(checkpoint_dir, 'latest_finetune.pth')\n",
    "\n",
    "# resuming_finetune = os.path.exists(resume_model)\n",
    "\n",
    "# if not resuming_finetune:\n",
    "#     resume_model = os.path.join(checkpoint_dir, 'latest.pth')\n",
    "\n",
    "# checkpoint = torch.load(resume_model, map_location=torch.device('cpu'), weights_only=False)\n",
    "# model.module.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "# saved_epoch = checkpoint['epoch']\n",
    "# start_epoch += saved_epoch\n",
    "# loss = checkpoint['loss']\n",
    "\n",
    "# if resuming_finetune:\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#     scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "#     min_loss, min_epoch = checkpoint['min_loss'], checkpoint['min_epoch']\n",
    "#     log_info = f'Resuming model from {resume_model}. resume_epoch: {saved_epoch}, min_loss: {min_loss:.4f}, min_epoch: {min_epoch}, loss: {loss:.4f}'\n",
    "#     logger.info(log_info)\n",
    "# else:\n",
    "#     min_loss = 0\n",
    "#     log_info = f'Loading baseline model from {resume_model}. resume_epoch: {saved_epoch}, loss: {loss:.4f}'\n",
    "#     logger.info(log_info)\n",
    "\n",
    "# print('#----------Training----------#')\n",
    "# print('iter_num=', len(train_loader))\n",
    "# for epoch in tqdm(range(start_epoch, config.epochs + 1), ncols=70):\n",
    "\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "#     train_one_epoch_multi(\n",
    "#         train_loader,\n",
    "#         model,\n",
    "#         optimizer,\n",
    "#         scheduler,\n",
    "#         epoch,\n",
    "#         logger,\n",
    "#         config,\n",
    "#         train_class_weights,\n",
    "#         scaler=scaler)\n",
    "\n",
    "#     print('#----------Validation----------#')\n",
    "#     loss = val_one_epoch_multi(\n",
    "#             val_loader,\n",
    "#             model,\n",
    "#             epoch,\n",
    "#             logger,\n",
    "#             config,\n",
    "#             val_class_weights)\n",
    "\n",
    "#     # if loss < min_loss and epoch > 35:\n",
    "#     if loss < min_loss:\n",
    "#         torch.save(model.module.state_dict(), os.path.join(checkpoint_dir, 'best_finetune.pth'))\n",
    "#         min_loss = loss\n",
    "#         min_epoch = epoch\n",
    "\n",
    "#     torch.save(\n",
    "#         {\n",
    "#             'epoch': epoch,\n",
    "#             'min_loss': min_loss,\n",
    "#             'min_epoch': min_epoch,\n",
    "#             'loss': loss,\n",
    "#             'model_state_dict': model.module.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'scheduler_state_dict': scheduler.state_dict(),\n",
    "#         }, os.path.join(checkpoint_dir, 'latest_finetune.pth'))\n",
    "\n",
    "# if os.path.exists(os.path.join(checkpoint_dir, 'best_finetune.pth')):\n",
    "#     print('#----------Testing----------#')\n",
    "#     best_weight = torch.load(config.work_dir + '/checkpoints/best_finetune.pth', map_location=torch.device('cpu'))\n",
    "#     model.module.load_state_dict(best_weight)\n",
    "#     loss = test_one_epoch_multi(\n",
    "#             test_loader,\n",
    "#             model,\n",
    "#             criterion,\n",
    "#             logger,\n",
    "#             config)\n",
    "#     os.rename(\n",
    "#         os.path.join(checkpoint_dir, 'best_finetune.pth'),\n",
    "#         os.path.join(checkpoint_dir, f'best_finetune-epoch{min_epoch}-loss{min_loss:.4f}.pth')\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNikfdTAh+wp6uP2KGSPuf/",
   "gpuType": "L4",
   "mount_file_id": "1yAhFbFsNwldGJ6LNuUgPid_Xqxe5qFDH",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
