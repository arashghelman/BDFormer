{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/content/drive/MyDrive/BDFormer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "QLGOu3JJY18L"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arashghelman/Desktop/Masters/Thesis/BDFormer/.conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/arashghelman/Desktop/Masters/Thesis/BDFormer/.conda/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-task dataset -> isic17\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from model.BDFormer import BDFormer\n",
    "from datasets import Skin_Dataset\n",
    "from engine import *\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from utils import *\n",
    "from config_setting import setting_config_multitask as config\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logger and output files are created.\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(config.work_dir + '/')\n",
    "\n",
    "log_dir = os.path.join(config.work_dir, 'log')\n",
    "checkpoint_dir = os.path.join(config.work_dir, 'checkpoints')\n",
    "resume_model = os.path.join(checkpoint_dir, 'latest.pth')\n",
    "outputs = os.path.join(config.work_dir, 'outputs')\n",
    "\n",
    "if not os.path.exists(checkpoint_dir): \n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "if not os.path.exists(outputs): \n",
    "    os.makedirs(outputs)\n",
    "\n",
    "if not os.path.exists(os.path.join(outputs, 'pred_masks')):\n",
    "    os.makedirs(os.path.join(outputs, 'pred_masks'))\n",
    "\n",
    "if not os.path.exists(os.path.join(outputs, 'pred_contours')):\n",
    "    os.makedirs(os.path.join(outputs, 'pred_contours'))\n",
    "\n",
    "global logger\n",
    "logger = get_logger('train', log_dir)\n",
    "log_config_info(config, logger)\n",
    "\n",
    "print('Logger and output files are created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is initalized. Device type: mps\n"
     ]
    }
   ],
   "source": [
    "if config.device.type == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "elif config.device.type == 'mps':\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "set_seed(config.seed, config.device)\n",
    "\n",
    "print(f'GPU is initalized. Device type: {config.device.type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting train set to 20 samples.\n",
      "Subsetting val set to 15 samples.\n",
      "Subsetting test set to 6 samples.\n",
      "Datasets are loaded.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Skin_Dataset(config, split=\"train\", subset_frac=0.01)\n",
    "train_class_weights = calculate_class_weights(train_dataset.labels, config.device)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=config.num_workers)\n",
    "\n",
    "val_dataset = Skin_Dataset(config, split=\"val\", subset_frac=0.1)\n",
    "val_class_weights = calculate_class_weights(val_dataset.labels, config.device)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=config.num_workers,\n",
    "    drop_last=True)\n",
    "\n",
    "test_dataset = Skin_Dataset(config, split=\"test\", subset_frac=0.01)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=config.num_workers,\n",
    "    drop_last=True)\n",
    "\n",
    "print('Datasets are loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinTransformerSys expand initial----depths:[2, 2, 2, 2];depths_decoder:[1, 2, 2, 2];drop_path_rate:0.2;num_classes:1\n",
      "Model and training parameters are configured.\n"
     ]
    }
   ],
   "source": [
    "model = BDFormer(img_size=256, in_channels=3, num_classes=config.num_classes, window_size=8).to(config.device)\n",
    "\n",
    "criterion = config.criterion\n",
    "optimizer = get_optimizer(config, model)\n",
    "scheduler = get_scheduler(config, optimizer)\n",
    "\n",
    "min_loss = 999\n",
    "start_epoch = 1\n",
    "min_epoch = 1\n",
    "\n",
    "print('Model and training parameters are configured.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4eqh3oZe_o7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------Training----------#\n",
      "iter_num= 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                           | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-task dataset ->multi-task dataset -> multi-task dataset -> multi-task dataset -> isic17isic17\n",
      "isic17\n",
      " isic17\n",
      "\n",
      "train: epoch 1, iter:0, loss: 2.0458, lr: 0.00033\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(resume_model):\n",
    "    print('#----------Resuming model and learning parameters----------#')\n",
    "    checkpoint = torch.load(resume_model, map_location=torch.device('cpu'), weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "    saved_epoch = checkpoint['epoch']\n",
    "    start_epoch += saved_epoch\n",
    "    min_loss, min_epoch, loss = checkpoint['min_loss'], checkpoint['min_epoch'], checkpoint['loss']\n",
    "\n",
    "    log_info = f'resuming model from {resume_model}. resume_epoch: {saved_epoch}, min_loss: {min_loss:.4f}, min_epoch: {min_epoch}, loss: {loss:.4f}'\n",
    "    logger.info(log_info)\n",
    "\n",
    "\n",
    "print('#----------Training----------#')\n",
    "print('iter_num=', len(train_loader))\n",
    "for epoch in tqdm(range(start_epoch, config.epochs + 1), ncols=70):\n",
    "    train_one_epoch_multi(\n",
    "        train_loader,\n",
    "        train_class_weights,\n",
    "        model,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        epoch,\n",
    "        logger,\n",
    "        config)\n",
    "\n",
    "    print('#----------Validation----------#')\n",
    "    loss = val_one_epoch_multi(val_loader, val_class_weights, model, epoch, logger, config)\n",
    "\n",
    "    if loss < min_loss:\n",
    "        torch.save(model.state_dict(), os.path.join(checkpoint_dir, 'best.pth'))\n",
    "        min_loss = loss\n",
    "        min_epoch = epoch\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            'epoch': epoch,\n",
    "            'min_loss': min_loss,\n",
    "            'min_epoch': min_epoch,\n",
    "            'loss': loss,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict()\n",
    "        }, os.path.join(checkpoint_dir, 'latest.pth'))\n",
    "\n",
    "if os.path.exists(os.path.join(checkpoint_dir, 'best.pth')):\n",
    "    print('#----------Testing----------#')\n",
    "    best_weight = torch.load(config.work_dir + '/checkpoints/best.pth', map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(best_weight)\n",
    "    loss = test_one_epoch_multi(test_loader, model, criterion, logger, config)\n",
    "    os.rename(\n",
    "        os.path.join(checkpoint_dir, 'best.pth'),\n",
    "        os.path.join(checkpoint_dir, f'best-epoch{min_epoch}-loss{min_loss:.4f}.pth')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing layers\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# for name, layer in model.multi_task_MaxViT.named_children():\n",
    "#     if name in ['classifier', 'seg_out_conv']:\n",
    "#         for param in layer.parameters():\n",
    "#             param.requires_grad = True\n",
    "\n",
    "# resume_model = os.path.join(checkpoint_dir, 'latest_finetune.pth')\n",
    "\n",
    "# resuming_finetune = os.path.exists(resume_model)\n",
    "\n",
    "# if not resuming_finetune:\n",
    "#     resume_model = os.path.join(checkpoint_dir, 'latest.pth')\n",
    "\n",
    "# checkpoint = torch.load(resume_model, map_location=torch.device('cpu'), weights_only=False)\n",
    "# model.module.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "# saved_epoch = checkpoint['epoch']\n",
    "# start_epoch += saved_epoch\n",
    "# loss = checkpoint['loss']\n",
    "\n",
    "# if resuming_finetune:\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#     scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "#     min_loss, min_epoch = checkpoint['min_loss'], checkpoint['min_epoch']\n",
    "#     log_info = f'Resuming model from {resume_model}. resume_epoch: {saved_epoch}, min_loss: {min_loss:.4f}, min_epoch: {min_epoch}, loss: {loss:.4f}'\n",
    "#     logger.info(log_info)\n",
    "# else:\n",
    "#     min_loss = 0\n",
    "#     log_info = f'Loading baseline model from {resume_model}. resume_epoch: {saved_epoch}, loss: {loss:.4f}'\n",
    "#     logger.info(log_info)\n",
    "\n",
    "# print('#----------Training----------#')\n",
    "# print('iter_num=', len(train_loader))\n",
    "# for epoch in tqdm(range(start_epoch, config.epochs + 1), ncols=70):\n",
    "\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "#     train_one_epoch_multi(\n",
    "#         train_loader,\n",
    "#         model,\n",
    "#         optimizer,\n",
    "#         scheduler,\n",
    "#         epoch,\n",
    "#         logger,\n",
    "#         config,\n",
    "#         train_class_weights,\n",
    "#         scaler=scaler)\n",
    "\n",
    "#     print('#----------Validation----------#')\n",
    "#     loss = val_one_epoch_multi(\n",
    "#             val_loader,\n",
    "#             model,\n",
    "#             epoch,\n",
    "#             logger,\n",
    "#             config,\n",
    "#             val_class_weights)\n",
    "\n",
    "#     # if loss < min_loss and epoch > 35:\n",
    "#     if loss < min_loss:\n",
    "#         torch.save(model.module.state_dict(), os.path.join(checkpoint_dir, 'best_finetune.pth'))\n",
    "#         min_loss = loss\n",
    "#         min_epoch = epoch\n",
    "\n",
    "#     torch.save(\n",
    "#         {\n",
    "#             'epoch': epoch,\n",
    "#             'min_loss': min_loss,\n",
    "#             'min_epoch': min_epoch,\n",
    "#             'loss': loss,\n",
    "#             'model_state_dict': model.module.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'scheduler_state_dict': scheduler.state_dict(),\n",
    "#         }, os.path.join(checkpoint_dir, 'latest_finetune.pth'))\n",
    "\n",
    "# if os.path.exists(os.path.join(checkpoint_dir, 'best_finetune.pth')):\n",
    "#     print('#----------Testing----------#')\n",
    "#     best_weight = torch.load(config.work_dir + '/checkpoints/best_finetune.pth', map_location=torch.device('cpu'))\n",
    "#     model.module.load_state_dict(best_weight)\n",
    "#     loss = test_one_epoch_multi(\n",
    "#             test_loader,\n",
    "#             model,\n",
    "#             criterion,\n",
    "#             logger,\n",
    "#             config)\n",
    "#     os.rename(\n",
    "#         os.path.join(checkpoint_dir, 'best_finetune.pth'),\n",
    "#         os.path.join(checkpoint_dir, f'best_finetune-epoch{min_epoch}-loss{min_loss:.4f}.pth')\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNikfdTAh+wp6uP2KGSPuf/",
   "gpuType": "L4",
   "mount_file_id": "1yAhFbFsNwldGJ6LNuUgPid_Xqxe5qFDH",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
